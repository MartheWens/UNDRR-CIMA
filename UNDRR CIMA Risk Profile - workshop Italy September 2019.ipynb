{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">  \n",
    "Notebook created by Hans De Moel and Marthe Wens <br>\n",
    "Within the UNDRR - CIMA Risk Profile Project\n",
    "</div>\n",
    "Drought Risk Assessment\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drought Hazard \n",
    "The **first part**  of this script will calculate different standardized drought indices for a time series of hydrological data (raster maps) with different accumulation times (for example for the cumulative sum over 3 or 6 months of Precipitation amounts).\n",
    "To identify droughts, a custom drought duration and intensity threshold can be chosen in the **second part** of the script; After defining the characteristics of 'a drought event', one can calculate the probabilities and return periods of droughts \n",
    "for different time periods. This will be done in the **third part** of the script.\n",
    " <br>\n",
    "## Drought Exposure \n",
    "To evaluate drought exposure, the created drought hazard files can be combined with exposure maps, as will be done in the **fourth part** of the script. Hence, once can calculate the annually average and probable maximum people, livestock, gdp, ... potentially affected by drought. This is done with a simple overlay and results are summed per admin1 regions (i.e. provinces).\n",
    " <br>\n",
    "## Drought Vulnerability \n",
    "We can try to estimate drought vulnerability by linking drought impacts (found in a PDNA) to the hazard indicators. Knowing how much percent of the exposed items (eg people, livestock) were severly affected (malnutrition, mortality), it is possible to  find a relation between severity of the drought hazard and experienced impact. This will be done in the **fifth part** of the script. As such, we can finetune the drought thresholds and use this relation to estimate drought impact in future periods (assuming no change in vulnerability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "Set the working directory to your the folder path on your computer, and choose the country you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('G:/PhD/UNDroughtProfiles')\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal, osr, ogr\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "import warnings\n",
    "#import FFTrees\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose country of analysis\n",
    "country = 'zambia' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data sources\n",
    "workingfolder = os.getcwd()\n",
    "inputfolder   = os.path.join(workingfolder,'Input')   #location of tif files\n",
    "datafolder    = os.path.join(workingfolder,'Data')    #location of nc files\n",
    "outputfolder  = os.path.join(workingfolder,'Output')  #location to save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Administrative levels to summarize exposure information              \n",
    "src = gdal.Open(os.path.join(inputfolder,country+'_lvl1_kron.tif'))\n",
    "Admin1 = src.GetRasterBand(1).ReadAsArray()                              \n",
    "admin = Admin1 - np.min(Admin1[Admin1>0]) +1 # number regions from 1 to x\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Paired')\n",
    "bounds = np.linspace(1, len(np.unique(admin)), len(np.unique(admin)+1))\n",
    "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "cmap.set_under(color='white')\n",
    "\n",
    "fig = plt.figure(figsize=(10,7)) \n",
    "plt.title(country+' administrative boundaries',size=25)\n",
    "im = plt.imshow(admin, cmap = cmap, norm=norm )  \n",
    "ax2 = fig.add_axes([0.95, 0.1, 0.03, 0.8])\n",
    "cb = matplotlib.colorbar.ColorbarBase(ax2, cmap=cmap, norm=norm,spacing='proportional', ticks=bounds, boundaries=bounds, format='%1i')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Read input data\n",
    "Here all input files (if they exist in your data folder) are automatically evaluated. All hydrological input data will be visualised. Please check if the file names are similar to the input files in your data folder, indicate the start and end period of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name input file ( * signifies that there are multiple; NAN if none)\n",
    "\n",
    "print('Precipitation data:')\n",
    "Prfiles  = os.path.join(datafolder,country+'.RainDaily.*.moncumulate*.nc')\n",
    "Pr       = xr.open_mfdataset(Prfiles, mask_and_scale=True, concat_dim='time', decode_times=True)\n",
    "print(Pr)\n",
    "\n",
    "print('\\nActual Evapotranspiration data:')\n",
    "EvTfiles = os.path.join(datafolder,country+'.ETRealDaily.*.moncumulate*.nc')\n",
    "Evt      = xr.open_mfdataset(EvTfiles, mask_and_scale=True, concat_dim='time', decode_times=True)\n",
    "print(Evt)\n",
    "\n",
    "print('\\nPotential Evaporation data:')\n",
    "PETfiles = os.path.join(datafolder,country+'.ETPotDaily.*.moncumulate*.nc')\n",
    "PET      = xr.open_mfdataset(PETfiles, mask_and_scale=True, concat_dim='time', decode_times=True)\n",
    "print(PET)\n",
    "\n",
    "print('\\nStreamflow data:')\n",
    "Qfiles   = os.path.join(datafolder,country+'.QDaily.*.moncumulate*.nc')\n",
    "Q        = xr.open_mfdataset(Qfiles, mask_and_scale=True, concat_dim='time', decode_times=True)\n",
    "print(Q)\n",
    "\n",
    "print('\\nSoil Moisture data:')\n",
    "SMfiles  = os.path.join(datafolder,country+'.SoilMoistureDaily.*.moncumulate*.nc')\n",
    "SM       = xr.open_mfdataset(SMfiles, mask_and_scale=True, concat_dim='time', decode_times=True)\n",
    "print(SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the time for which data is available\n",
    "start = 1951\n",
    "end   = 2005\n",
    "years = end - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean(indata,title):\n",
    "\n",
    "    info = np.nanmean(indata, axis=0)*12\n",
    "    minval = np.nanpercentile(info[info!=0],5)\n",
    "    maxval = np.nanpercentile(info[info!=0],95)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    plt.title(title+' ', size=20)   \n",
    "    im = plt.imshow(info, cmap = matplotlib.cm.get_cmap('YlGnBu'), vmin=minval, vmax=maxval)\n",
    "    im.axes.get_xaxis().set_visible(False)\n",
    "    im.axes.get_yaxis().set_visible(False)\n",
    "    cbar = fig.colorbar(im)\n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "    fig.savefig(os.path.join(outputfolder,country+title+'.png'))  \n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input dictionary and show maps of input\n",
    "parameters = {}                                                          \n",
    "\n",
    "parameters['streamflow']    = np.transpose(Q.Qmap.values, (0, 2, 1))\n",
    "plot_mean(parameters['streamflow'], country+' mean streamflow')\n",
    "\n",
    "parameters['soilmoisture']  = np.transpose(SM.WatVol_RZ.values, (0, 2, 1))\n",
    "plot_mean(parameters['soilmoisture'], country+' mean soilmoisture')\n",
    "\n",
    "parameters['precipitation'] = np.transpose(Pr.Rain.values, (0, 2, 1))\n",
    "plot_mean(parameters['precipitation'], country+' mean precipitation')\n",
    "\n",
    "parameters['pr - potevt']   = np.transpose(Pr.Rain.values, (0, 2, 1)) - np.transpose(PET.DailyETP_PMFAO.values, (0, 2, 1))\n",
    "plot_mean(parameters['pr - potevt'], country+' mean effective rainfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Calculate drought indices\n",
    "Here we can calculate the standardized indices following the method explained in the powerpoint. **We did this already for you since it takes a lot of time, so you do NOT need to run these.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose standardized indices used in analysis\n",
    "\n",
    "indices = ['SPEI', 'SPI','SSMI','SSFI']\n",
    "indicesfull = ['Standardized Precipitation Evapotranspiration Index','Standardized Precipitation Index','Standardized Soil Moisture Index','Standardized Streamflow Index']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hydrological data accumulation time \n",
    "    \n",
    "accumulationtimes = [1,3,6,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store indices output\n",
    "standardizedindices = {}\n",
    "standardizedindices['SPEI'] = 'pr - potevt'\n",
    "standardizedindices['SPI']  = 'precipitation'\n",
    "standardizedindices['SSFI'] = 'streamflow'\n",
    "standardizedindices['SSMI'] = 'soilmoisture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare netCDF output\n",
    "variables = {}       \n",
    "variables['time'] = Pr['time']\n",
    "variables['lon'] = Pr['lat']     \n",
    "variables['lat'] = Pr['lon'] # Transposed\n",
    "variables['attrs'] = Pr.Rain.attrs\n",
    "variables['dims'] = Pr.Rain.dims\n",
    "variables['coords'] = Pr.Rain.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to accumulate hydrological data\n",
    "   # with a the input data and b the accumulation time\n",
    "   # -> the accumulated value coincidences with the position of the last value \n",
    "   #     used in the accumulation process.\n",
    "\n",
    "def moving_sum(a, b) :\n",
    "    \n",
    "    cummuldata = np.cumsum(a, dtype=float)                 \n",
    "    cummuldata[b:] = cummuldata[b:] - cummuldata[:-b]         \n",
    "    cummuldata[:b - 1] = np.nan                                           \n",
    "    \n",
    "    return cummuldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate standardized indices per month\n",
    "    # with a the full accumulated data series over which the index is calculated\n",
    "    # and b the standardized index that is analysed\n",
    "\n",
    "def calculate_Index(a, b):\n",
    "      \n",
    "    indexvalues = a * np.nan                        \n",
    "    for m in range(12):  \n",
    "     \n",
    "        # Extract monthly values\n",
    "        monthlyvalues = np.zeros(int(len(a)/12)+1) * np.nan\n",
    "        for yr in range(int(len(a)/12)): \n",
    "            monthlyvalues[yr] = accumulateddata[(12*yr)+m]                                 \n",
    "                                \n",
    "        # Retrieve index per month\n",
    "        Zval = calculate_Zvalue(monthlyvalues,b)\n",
    "                            \n",
    "        # Reconstruct time series\n",
    "        for yr in range(int(len(a)/12)):\n",
    "            indexvalues[(12*yr)+m] = Zval[yr]\n",
    "            \n",
    "    return indexvalues     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Z values for a time series of one selected month\n",
    "    # with a the data series over which the index is calculated\n",
    "    # and b the standardized index that is analysed \n",
    "                            \n",
    "def calculate_Zvalue(a, b):\n",
    "        \n",
    "    # extract reference time series\n",
    "    referenceseries = a[:50]    \n",
    "    \n",
    "    # find fitting distribution for reference sereis\n",
    "    best_dist, best_p, params = get_best_distribution(referenceseries, b)  \n",
    "    \n",
    "    # fit full time series over best distribution\n",
    "    z = np.zeros(len(a))\n",
    "    dist = getattr(stats, str(best_dist))                                   \n",
    "    rv = dist(*params)         \n",
    "        \n",
    "    # Create suitable cummulative distribution function\n",
    "    # Solve issue with zero values in Gamma distribution (cfr.Stagge et al. 2015)\n",
    "    if dist == 'gamma':                                                     \n",
    "        nyears_zero = len(a) - np.count_nonzero(a)\n",
    "        p_zero = nyears_zero / len(a)\n",
    "        p_zero_mean = (nyears_zero + 1) / (2 * (len(a) + 1))           \n",
    "\n",
    "        ppd = (a * 0 ) + p_zero_mean\n",
    "        ppd[np.nonzero(a)] = p_zero+((1-p_zero)*rv.cdf(a[np.nonzero(a)]))\n",
    "       \n",
    "    else:\n",
    "        ppd = rv.cdf(a)\n",
    "    \n",
    "    # Standardize the fitted cummulative distribtuion distribution \n",
    "    z = stats.norm.ppf(ppd)                                   \n",
    "    \n",
    "    # limit extreme, unlikely values \n",
    "    z[z>3] = 3\n",
    "    z[z<-3] = -3 \n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the best fitting statistical distribution\n",
    "    # with a the reference time series to test the distributions \n",
    "    # and b the standardized index that is analysed\n",
    "    # (possible distributions differ per hydrological data source)\n",
    "\n",
    "def get_best_distribution(a, b):\n",
    "    \n",
    "    if b == 'SPEI':                     # Suggestions by Stagge et al. (2015) \n",
    "        dist_names = ['norm','genextreme', 'genlogistic', 'pearson3']                  \n",
    "    elif b == 'SSMI':                   # Suggestions in Ryu et al. (2005)\n",
    "        dist_names = ['norm','beta',  'pearson3','fisk']                               \n",
    "    elif b == 'SPI' :                   # Suggestions by Stagge et al. (2015) \n",
    "        dist_names = ['norm','gamma', 'weibull_min', 'logistic']\n",
    "    elif b == 'SSFI':                   # Suggestions by Vincent_Serrano et al. (2012)\n",
    "        dist_names = ['weibull_min','lognorm', 'pearson3', 'genextreme'] \n",
    "    else:\n",
    "        print('problem finding distribution')\n",
    "\n",
    "    # find fit for each optional distribution\n",
    "    dist_results = []\n",
    "    params = {}\n",
    "    for dist_name in dist_names:                                                # Find distribution parameters        \n",
    "        dist = getattr(stats, dist_name)\n",
    "        param = dist.fit(a)\n",
    "        params[dist_name] = param\n",
    "        \n",
    "        # Assess goodness-of-fit using Kolmogorovâ€“Smirnov test\n",
    "        D, p = stats.kstest(a, dist_name, args=param)                      # Applying the Kolmogorov-Smirnov test\n",
    "        dist_results.append((dist_name, p))\n",
    "  \n",
    "    # find best fitting statistical distribution\n",
    "    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))           # Select the best fitted distribution\n",
    "\n",
    "    return best_dist, best_p, params[best_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save output to netCDF format\n",
    "    # with a input data source\n",
    "    # and b output name\n",
    "         \n",
    "def save_to_nc(a, b, varis):\n",
    "  \n",
    "    da = xr.DataArray(a,name=b,coords=varis['coords'],dims=varis['dims'],attrs=varis['attrs'])\n",
    "    ds = da.to_dataset(name=b)\n",
    "    nc_out = os.path.join(os.path.join(outputfolder,b+'.nc4'))\n",
    "    ncx = ds.to_netcdf(nc_out,engine='scipy')\n",
    "\n",
    "    return ncx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN SCRIPT ##\n",
    "# Loops over all indices and all accumulation times\n",
    "\n",
    "for i, index in enumerate(indices):   \n",
    "         \n",
    "    param = standardizedindices[index]\n",
    "    print('analyse '+param) \n",
    "    \n",
    "    data = parameters[param]\n",
    "    plot_maps(data, country+' '+param, matplotlib.cm.get_cmap('YlGnBu'))\n",
    "    \n",
    "    for acc in range(len(accumulationtimes)): \n",
    "        accumulation = accumulationtimes[acc]\n",
    "        \n",
    "        print('calculate '+index+str(accumulation))\n",
    "            \n",
    "        # Prepare empty output files\n",
    "        StandardizedIndex  = np.zeros((data.shape[0],data.shape[1],data.shape[2])) * np.nan \n",
    "               \n",
    "        # Loop over all pixels\n",
    "        for lo in range(len(variables['lon'])):\n",
    "            \n",
    "            print(int(100 * lo / len(variables['lon'])),\"%\") \n",
    "            \n",
    "            for la in range(len(variables['lat'])):\n",
    "                    \n",
    "                # Select data per pixel and month  \n",
    "                pixel = data[:,lo,la] \n",
    "                pixel[pixel == -999] = np.nan # wrong data\n",
    "                    \n",
    "                # Make sure no false data is there (Nan, zeroes)\n",
    "                if np.isnan(pixel).sum() < 1 :#and np.max(pixel) > 10:                             \n",
    "                        \n",
    "                    # Calculate accumulated dataset  ! accumulation leaves an incomplete first year;\n",
    "                    accumulateddata = moving_sum(pixel,accumulation)\n",
    "\n",
    "                    # Calculate standardized index\n",
    "                    StandardizedIndex[12:,lo,la] = calculate_Index(accumulateddata[12:], index)\n",
    "                                  \n",
    "        # save and plot Standardized Index\n",
    "        nameout =  country+' '+index+str(accumulation)          \n",
    "        nc = save_to_nc(np.transpose(StandardizedIndex, (0, 2, 1)), nameout, variables)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot three diverse maps to present the data\n",
    "    # with a the data to present\n",
    "    # and b the title\n",
    "    \n",
    "def plot_maps(a, b, c):\n",
    "                  \n",
    "    #spatial average per month\n",
    "\n",
    "    yearlymean = np.nanmean(np.nanmean(StandardizedIndex, axis=(1)), axis=(1)).tolist()\n",
    "    minval=yearlymean.index(np.nanmin(yearlymean))\n",
    "    medval=yearlymean.index(np.sort(yearlymean)[int(len(yearlymean)/2)])\n",
    "    maxval=yearlymean.index(np.nanmax(yearlymean))\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize=(30,10))  \n",
    "    \n",
    "    ax1 = plt.subplot(1,3,1)    \n",
    "    im = ax1.imshow(StandardizedIndex[maxval,:,:], cmap = c, vmin = -3, vmax = 3) \n",
    "    ax1.set_title('WET month', size=28)\n",
    "    im.axes.get_xaxis().set_visible(False)\n",
    "    im.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax2 = plt.subplot(1,3,2)    \n",
    "    im = ax2.imshow(StandardizedIndex[medval,:,:], cmap = c, vmin = -3, vmax = 3)\n",
    "    ax2.set_title('NORMAL month', size=28)        \n",
    "    im.axes.get_xaxis().set_visible(False)\n",
    "    im.axes.get_yaxis().set_visible(False)\n",
    "                 \n",
    "    ax3 = plt.subplot(1,3,3)\n",
    "    im = ax3.imshow(StandardizedIndex[minval,:,:], cmap =  c, vmin = -3, vmax = 3) \n",
    "    ax3.set_title('DRY month', size=28)\n",
    "    im.axes.get_xaxis().set_visible(False)\n",
    "    im.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "    \n",
    "    plt.suptitle(b,size=40)\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(outputfolder,b+'_3mapsN.png'))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise indices maps\n",
    "\n",
    "ix = 'SPI' # choose index to visualise\n",
    "ac = '3' # choose accumulation time to visualise\n",
    "infile  = xr.open_dataset(os.path.join(outputfolder,country+' '+ix+str(ac)+'.nc4'), mask_and_scale=True, decode_times=True)\n",
    "StandardizedIndex = np.transpose(infile[country+' '+ix+str(ac)].values, (0, 2, 1))\n",
    "        \n",
    "plot_maps(StandardizedIndex, country+' '+ix+ac, matplotlib.cm.get_cmap('RdYlBu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Choose Drought Threshold \n",
    "Here are different methods to define your droughts; ways to verify the minimum intensity and minimum duration of a dry spell to be characterised as a drought. An easy approach is the climate-dependent threshold, which is automatically generated based on the aridity index. We can also manualy (in excel) verify the thresholds that are exceeded when impacts are experienced. Lastly, there is an automatic, machine learning technique to find the thresholds (not in the script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose standardized indices used in threshold analysis\n",
    "\n",
    "ixs = ['SPI','SPEI','SSMI'] # choose index to visualise\n",
    "acs = ['3','6'] # choose accumulation time to visualise\n",
    "\n",
    "Indices_dict = {}\n",
    "for ix in ixs:\n",
    "    for ac in acs :\n",
    "        infile  = xr.open_dataset(os.path.join(outputfolder,'zambesi_'+ix+str(ac)+'.nc4'), mask_and_scale=True, decode_times=True)\n",
    "        Indices_dict['{}_{}'.format(ix,ac)] = infile['zambesi_'+ix+str(ac)].values\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export all standardized values per admin regio to excel to evaluate threshold choice\n",
    "\n",
    "# Import file with admin1 regions\n",
    "src = gdal.Open(os.path.join(datafolder, \"Zambesi/ZM_lvl1_final.tif\"))\n",
    "mask = src.GetRasterBand(1).ReadAsArray()\n",
    "mask = np.flipud(mask)\n",
    "\n",
    "# Define some general parts for the excel file\n",
    "Admin_names = ['Central', 'Copperbelt', 'Eastern', 'Luapula', 'Lusaka', 'Muchinga', 'North-Western', 'Northern', 'Southern', 'Western']\n",
    "Header_names = ['Year', 'Month', 'Drought'] + list(Indices_dict.keys())\n",
    "Years = np.kron(list(range(1979, 2019)),np.ones((1,12))).flatten()\n",
    "Months = list(range(1,13))*40\n",
    "workbook = xlsxwriter.Workbook(os.path.join(outputfolder,'Zambezi_Drought_Indices.xlsx'), {'nan_inf_to_errors': True})\n",
    "\n",
    "#Loop over admin1 regions (to different sheets)\n",
    "for nr, a in enumerate(Admin_names):\n",
    "    admin_mask = mask == nr+1\n",
    "    row=0\n",
    "    col=0\n",
    "    #Evaluate mean values per admin1 and create table for excel\n",
    "    for_excel = pd.DataFrame()        \n",
    "    for i, ind in enumerate(Indices_dict):\n",
    "        forsize = Indices_dict[ind].shape[0]\n",
    "        forpandas = []\n",
    "        for month in range(forsize):\n",
    "            data = Indices_dict[ind][month,:,:]\n",
    "            forpandas.append(np.nanmean(data[admin_mask]))\n",
    "        for_excel[ind] = forpandas          \n",
    "    #Write to Excel\n",
    "    worksheet = workbook.add_worksheet(a)\n",
    "    for i, name in enumerate(Header_names):\n",
    "        worksheet.write(row,col+i,name)\n",
    "    for i, name in enumerate(Years):\n",
    "        worksheet.write(row+1+i,col,name)\n",
    "    for i, name in enumerate(Months):\n",
    "        worksheet.write(row+1+i,col+1,name)\n",
    "    for col, data in enumerate(for_excel.values.T):\n",
    "        worksheet.write_column(row+1, col+3, data)\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose drought threshold \n",
    "intensitythreshold = -1 # one number, \"climate-dependent\", \"regio-specific\"  \n",
    "durationthreshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if intensitythreshold == 'climate-dependent':\n",
    "\n",
    "Pr  = np.transpose(xr.open_mfdataset(os.path.join(data,'Zambesi/ETcumPot_AF*.nc'), mask_and_scale=True, concat_dim='time', decode_times=True).Rain.values, (0, 2, 1))\n",
    "PET = np.transpose(xr.open_mfdataset(os.path.join(data,'Zambesi/Rain_AF*.nc'), mask_and_scale=True, concat_dim='time', decode_times=True).DailyETP_PMFAO.values, (0, 2, 1))\n",
    "    \n",
    "AridityIndex  = ( np.mean(Pr, axis=0) * 365 ) / ( np.mean(PET, axis=0) * 365 )\n",
    "    \n",
    "Droughtthresholds = Pr * np.nan\n",
    "Droughtthresholds =  -0.9 * AridityIndex - 0.8\n",
    "Droughtthresholds[Droughtthresholds>-0.5] = -0.5\n",
    "Droughtthresholds[Droughtthresholds<-1.8] = -1.8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if intensitythreshold == 'regio-specific':\n",
    "Droughtthresholds = mask\n",
    "    \n",
    "Droughtthresholds[mask==1] = -1.0  # threshold value for Central\n",
    "Droughtthresholds[mask==2] = -1.1  # threshold value for Copperbelt\n",
    "Droughtthresholds[mask==3] = -1.5  # threshold value for Eastern\n",
    "Droughtthresholds[mask==4] = -1.3  # threshold value for Luapula\n",
    "Droughtthresholds[mask==5] = -1.2  # threshold value for Lusaka\n",
    "Droughtthresholds[mask==6] = -1.4  # threshold value for Muchinga\n",
    "Droughtthresholds[mask==7] = -1.6  # threshold value for North-Western\n",
    "Droughtthresholds[mask==8] = -1.8  # threshold value for Northern\n",
    "Droughtthresholds[mask==9] = -2.0  # threshold value for Southern\n",
    "Droughtthresholds[mask==10] = -1.7 # threshold value for Western"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fixed value for full country\n",
    "Droughtthresholds = mask    \n",
    "Droughtthresholds[mask>0] =  -1 # threshold value for full country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save drought threshold map\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "plt.title('Drought Intensity Threshold', size=40)   \n",
    "im = plt.imshow(np.flipud(Droughtthresholds), cmap = matplotlib.cm.get_cmap('plasma_r'), clim=(-2,-0.5))\n",
    "im.axes.get_xaxis().set_visible(False)\n",
    "im.axes.get_yaxis().set_visible(False)\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.ax.tick_params(labelsize=20) \n",
    "fig.savefig(os.path.join(outputfolder,'Thresholdmap.png'))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assess Drought Hazard\n",
    "After choosing the intensity and duration thresholds, the probability maps can be created for the chosen indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose standardized indices used in hazard analysis\n",
    "\n",
    "indices = ['SPEI']\n",
    "indicesfull = ['Standardized Precipitation Evapotranspiration Index','Standardized Precipitation Index','Standardized Soil Moisture Index','Standardized Streamflow Index']  \n",
    "\n",
    "accumulationtimes = ['3']\n",
    "\n",
    "periods     = ['past climate']\n",
    "periodstart = [1970] \n",
    "periodsend  = [2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify drought events\n",
    "    # with a the standardized values and b the drougth threshold\n",
    "    # ! nothing is done with severity and drought months - possible future extension\n",
    "\n",
    "def calculate_Probability(a, b):\n",
    "    \n",
    "    a = a[~np.isnan(a)]\n",
    "\n",
    "    # If higher than intensity threshold: call it drought month\n",
    "    duration = a * 0\n",
    "    deficit = a * 0\n",
    "    for t in range(1,len(a)):\n",
    "        if a[t] <= b:\n",
    "            duration[t] = duration[t-1] + 1\n",
    "            deficit[t] = deficit[t-1] + ( b - a[t] )\n",
    "        else:\n",
    "            duration[t] = 0\n",
    "            deficit[t] = 0\n",
    "                  \n",
    "    # If higher than time threshold: call it drought event\n",
    "    droughtevents = np.sum(duration == durationthreshold) \n",
    "    \n",
    "    # Option to work with deficit\n",
    "    #droughtevents = np.sum(deficit == minimummonths) \n",
    "\n",
    "    # Calculate probability of occurence of drought event per year\n",
    "    zt = (100 * ( droughtevents / (len(a)/12) ) )+ 1           \n",
    "           \n",
    "    # Calculate return period of drought events (in months)\n",
    "    #zp = ((len(a)/12) + 1 ) / droughtevents\n",
    "    \n",
    "    return droughtevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save hazard output to jpg format\n",
    "    # with a input data source\n",
    "    # and b output name\n",
    "    \n",
    "def hazard_plot(a,b,c,d,e):\n",
    "    \n",
    "    # create color map\n",
    "    cmap = matplotlib.cm.get_cmap(\"OrRd\")\n",
    "    color_list = cmap(np.linspace(0, 1, 26))[2:,:-1]\n",
    "    cmap_name = cmap.name + str(11)\n",
    "    cmap = cmap.from_list(cmap_name, color_list, 25)\n",
    "    cmap.set_bad(color='white')\n",
    "    cmap.set_under(color='grey')\n",
    "    \n",
    "    data = np.ma.masked_where(np.isnan(a), a)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,8)) \n",
    "    plt.title(b, size=32) \n",
    "    im = plt.imshow(data, cmap = cmap, vmin= 0, vmax=30)\n",
    "    \n",
    "    cbar = fig.colorbar(im, ticks = [0,5,10,15,20,25,30], orientation=\"vertical\")\n",
    "    cbar.set_label(\"drought probability (%)\", size=14)\n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "    \n",
    "    im.axes.get_xaxis().set_visible(False)\n",
    "    im.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    text =\"Annual average chance of having three or more drought months\\nbased on the \"+c+\"\\n with accumulated data of \"+d+\" months\"\n",
    "    fig.text(0.10,0.12, text, fontsize=18, ha='left', va='top', wrap=True)\n",
    "            \n",
    "    fig.savefig(os.path.join(outputfolder,e+'.png'))\n",
    "    plt.plot()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save output to TIF format\n",
    "    # with a input data source\n",
    "    # and b output name\n",
    "def save_to_tif(a, b, varis): \n",
    "\n",
    "    cols        = a.shape[1]\n",
    "    rows        = a.shape[0]\n",
    "    originX     = np.min(varis['lat']) #top left point\n",
    "    originY     = np.max(varis['lon']) #top left point\n",
    "    pixelWidth  = (np.max(varis['lat'])-np.min(varis['lat']))/(varis['lat'].shape[0]-1)\n",
    "    pixelHeight = - (np.max(varis['lon'])-np.min(varis['lon']))/(varis['lon'].shape[0]-1)\n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    outRaster = driver.Create(b, cols, rows, 1, gdal.GDT_Byte)\n",
    "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "    outband = outRaster.GetRasterBand(1)\n",
    "    outband.WriteArray(a)\n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromEPSG(4326)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN SCRIPT #\n",
    "\n",
    "for i, index in enumerate(indices):   \n",
    "                \n",
    "    for acc in range(len(accumulationtimes)): \n",
    "        accumulation = accumulationtimes[acc]\n",
    "        print('calculate hazard '+index+str(accumulation))\n",
    "                   \n",
    "        # Open Indices file:\n",
    "        infile  = xr.open_dataset(os.path.join(outputfolder,'zambesi_'+index+str(accumulation)+'.nc4'), mask_and_scale=True, decode_times=True)\n",
    "        StandardizedIndex = infile['zambesi_'+index+str(accumulation)].values\n",
    "\n",
    "        # Prepare empty output files\n",
    "        DroughtHazardMap   = np.zeros((len(periods),StandardizedIndex.shape[1],StandardizedIndex.shape[2])) * np.nan\n",
    "        \n",
    "        variables = {}       \n",
    "        variables['time'] = infile['time']\n",
    "        variables['lon'] = infile['lon']  \n",
    "\n",
    "        variables['lat'] = infile['lat'] \n",
    "\n",
    "        # Loop over all pixels\n",
    "        for lo in range(len(infile['lon'])):\n",
    "         \n",
    "            for la in range(len(infile['lat'])):\n",
    "                    \n",
    "                # Select data per pixel and month  \n",
    "                pixel = StandardizedIndex[:,la,lo]                            \n",
    "                \n",
    "                if np.isnan(pixel).sum() < 100 :\n",
    "               \n",
    "                    for per, period in enumerate(periods):                         \n",
    "                        \n",
    "                        # Calculate probabilities FOR FULL period ! Adjust if future data is there...\n",
    "                        DroughtHazardMap[per,la,lo] = calculate_Probability(pixel, Droughtthresholds[la,lo])       \n",
    "\n",
    "        # save and visualise Hazard Map\n",
    "        for per, period in enumerate(periods): \n",
    "            nameout =  country+'_hazard_'+index+str(accumulation)+'_'+period     \n",
    "            name = country+' drought hazard \\n'+index+str(accumulation)+' '+period \n",
    "            save_to_tif(np.asarray(np.flipud(DroughtHazardMap[per,:,:])), os.path.join(outputfolder,nameout+'.tif'), variables)\n",
    "            hazard_plot(np.flipud(DroughtHazardMap[per,:,:]), name, indicesfull[i], str(accumulation),nameout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
